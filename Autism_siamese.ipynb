{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Autism_siamese.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sutummala/AutismNet/blob/main/Autism_siamese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iq5akkk4CpnB",
        "outputId": "00400e5f-e8b4-45da-a56e-847da33287f1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7g0odBW6RA-"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "#import tensorflow_addons as tfa\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
        "import sklearn\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_grad_cam import GradCAM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEuSRuMW6j15"
      },
      "source": [
        "# normalizing the input to have values between zero and one to make them suitable for further analysis\n",
        "def normalize(input):\n",
        "  norm_input = []\n",
        "  print(f'shape of input is {np.shape(input[0])}')\n",
        "  for i in range(np.shape(input)[0]):\n",
        "    norm_in = (input[i]-np.min(input[i]))/(np.max(input[i])-np.min(input[i]))\n",
        "    norm_input.append(norm_in)\n",
        "  return norm_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6gWYWB3g--B",
        "outputId": "9c8fe199-5962-4d89-de00-5f7daac0867b"
      },
      "source": [
        "left_input = np.load('/content/drive/My Drive/Autism_CNN/left_input.npy')\n",
        "left_input = np.squeeze(normalize(left_input))\n",
        "\n",
        "right_input = np.load('/content/drive/My Drive/Autism_CNN/right_input.npy')\n",
        "right_input = np.squeeze(normalize(right_input))\n",
        "\n",
        "targets = np.load('/content/drive/My Drive/Autism_CNN/autism_labels.npy')\n",
        "\n",
        "targets = 1-targets # 1 for negative pair and 0 for positive pair\n",
        "\n",
        "print(f'total size of the data is {len(targets)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of input is (30, 45, 30)\n",
            "shape of input is (30, 45, 30)\n",
            "total size of the data is 4280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZKLD3_hNIM7",
        "outputId": "e1c47ccf-18f7-4865-ad7b-98f24597e66b"
      },
      "source": [
        "folds = RepeatedStratifiedKFold(n_splits = 5, n_repeats = 1)\n",
        "\n",
        "for train_index, test_index in folds.split(left_input, targets):\n",
        "    left_input_cv, left_input_test, targets_cv, targets_test = left_input[train_index], left_input[test_index], targets[train_index], targets[test_index]\n",
        "    right_input_cv, right_input_test = right_input[train_index], right_input[test_index]\n",
        "\n",
        "#cv_index = int(0.9 * len(targets)) \n",
        "\n",
        "#left_input_cv = left_input[:cv_index]\n",
        "#right_input_cv = right_input[:cv_index]\n",
        "#targets_cv = targets[:cv_index]\n",
        "\n",
        "print(f'shape of left/right input for CV is {left_input_cv.shape}')\n",
        "print(f'input size for cross-validation is {len(targets_cv)}')\n",
        "print(f'no.of positive pairs in CV are {np.shape(np.nonzero(targets_cv))[1]}')\n",
        "\n",
        "#left_input_test = left_input[cv_index:]\n",
        "#right_input_test = right_input[cv_index:]\n",
        "#targets_test = targets[cv_index:]\n",
        "\n",
        "print(f'shape of left/right input for testing is {left_input_test.shape}')\n",
        "print(f'input size for testing is {len(targets_test)}')\n",
        "print(f'no.of positive pairs in test are {np.shape(np.nonzero(targets_test))[1]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of left/right input for CV is (3424, 30, 45, 30)\n",
            "input size for cross-validation is 3424\n",
            "no.of positive pairs in CV are 1694\n",
            "shape of left/right input for testing is (856, 30, 45, 30)\n",
            "input size for testing is 856\n",
            "no.of positive pairs in test are 423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_Z6p5sLNprz"
      },
      "source": [
        "def specificity(y_true, y_pred):\n",
        "    \n",
        "    y_pred = K.cast(y_pred > 0.5, y_true.dtype)\n",
        "    tn, fp, fn, tp = (0, 0, 0, 0)\n",
        "    for i in range(len(y_true)):\n",
        "      if y_true[i] == 1 and y_pred[i] == 1:\n",
        "        tp += 1\n",
        "      elif y_true[i] == 1 and y_pred[i] == 0:\n",
        "        fp += 1\n",
        "      elif y_true[i] == 0 and y_pred[i] == 1:\n",
        "        fn += 1\n",
        "      elif y_true[i] == 0 and y_pred[i] == 0:\n",
        "        tn += 1\n",
        "    return tn/(tn+fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFHiMSJcQC9h"
      },
      "source": [
        "def contrastive_loss(y_true, y_pred):\n",
        "    margin = 1\n",
        "    square_pred = K.square(y_pred)\n",
        "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
        "    return K.mean((1-y_true) * square_pred + y_true * margin_square)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBFMSWEyNJ6g"
      },
      "source": [
        "## newly added methods begin\n",
        "def recall_m(y_true, y_pred):\n",
        "  y_pred = K.cast(y_pred > 0.5, y_true.dtype)\n",
        "  true_positives = (K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "  possible_positives = (K.round(K.clip(y_true, 0, 1)))\n",
        "  recall = K.mean(K.equal(true_positives, possible_positives))\n",
        "  return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "  y_pred = K.cast(y_pred > 0.5, y_true.dtype)\n",
        "  true_positives = (K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "  predicted_positives = (K.round(K.clip(y_pred, 0, 1)))\n",
        "  precision = K.mean(K.equal(true_positives, predicted_positives))\n",
        "  return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "  precision = precision_m(y_true, y_pred)\n",
        "  recall = recall_m(y_true, y_pred)\n",
        "  return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "##newly added end\n",
        "\n",
        "def compute_accuracy(y_true, y_pred):\n",
        "    '''Compute classification accuracy with a fixed threshold on distances.\n",
        "    '''\n",
        "    pred = y_pred.ravel() < 0.5\n",
        "    return np.mean(pred == y_true)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    '''Compute classification accuracy with a fixed threshold on distances.\n",
        "    '''\n",
        "    return K.mean(K.equal(y_true, K.cast(y_pred > 0.5, y_true.dtype)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B605Y_SfGXpL"
      },
      "source": [
        "def euclidean_distance(vectors):\n",
        "\t# unpack the vectors into separate lists\n",
        "\t(featsA, featsB) = vectors\n",
        "\t# compute the sum of squared distances between the vectors\n",
        "\tsumSquared = K.sum(K.square(featsA - featsB), axis=1,\n",
        "\t\tkeepdims=True)\n",
        "\t# return the euclidean distance between the vectors\n",
        "\treturn K.sqrt(K.maximum(sumSquared, K.epsilon()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTwj0FaeNStL"
      },
      "source": [
        "def eucl_dist_output_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0], 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbOUy2GR9TWn"
      },
      "source": [
        "def specificity1(y_true, y_pred):\n",
        "    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
        "    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n",
        "    return true_negatives / (possible_negatives + K.epsilon())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QYTMFZ8hbK7"
      },
      "source": [
        "def SiameseNetwork(input_shape):\n",
        "    \n",
        "    input_layer = tf.keras.Input(input_shape)\n",
        "\n",
        "   # 1st 3D conv blocks, which involves, convolution, BN, activation and pooling \n",
        "    x_1 = tf.keras.layers.Conv3D(32, (3,3,3), strides = (1,1,1), padding = 'same', kernel_regularizer = 'L2', name = 'conv3d_1')(input_layer)\n",
        "    x_1_bn = tf.keras.layers.BatchNormalization(axis = -1)(x_1)\n",
        "    x_1_bn_ac = tf.keras.layers.Activation('relu')(x_1_bn)\n",
        "\n",
        "    x_2 = tf.keras.layers.Conv3D(32, (3,3,3), strides = (1,1,1), padding = 'same', kernel_regularizer = 'L2', name = 'conv3d_2')(x_1_bn_ac)\n",
        "    x_2_bn = tf.keras.layers.BatchNormalization(axis = -1)(x_2)\n",
        "    x_2_bn_ac = tf.keras.layers.Activation('relu')(x_2_bn)\n",
        "    x_2_bn_ac_pooling = tf.keras.layers.MaxPooling3D(strides = (2, 2, 2))(x_2_bn_ac)\n",
        "              \n",
        "    # 2nd 3D conv block, which involves, convolution, BN, activation and pooling \n",
        "    x_3 = tf.keras.layers.Conv3D(64, (3,3,3), strides = (1,1,1), padding = 'same', kernel_regularizer = 'L2', name = 'conv3d_3')(x_2_bn_ac_pooling)\n",
        "    x_3_bn = tf.keras.layers.BatchNormalization(axis = -1)(x_3)\n",
        "    x_3_bn_ac = tf.keras.layers.Activation('relu')(x_3_bn)\n",
        "\n",
        "    x_4 = tf.keras.layers.Conv3D(64, (3,3,3), strides = (1,1,1), padding = 'same', kernel_regularizer = 'L2', name = 'conv3d_4')(x_3_bn_ac)\n",
        "    x_4_bn = tf.keras.layers.BatchNormalization(axis = -1)(x_4)\n",
        "    x_4_bn_ac = tf.keras.layers.Activation('relu')(x_4_bn)\n",
        "    x_4_bn_ac_pooling = tf.keras.layers.MaxPooling3D(strides = (2, 2, 2))(x_4_bn_ac)\n",
        "    \n",
        "    # 3rd 3D conv block, which involves, convolution, BN, activation and pooling \n",
        "    x_5 = tf.keras.layers.Conv3D(256, (3,3,3), strides = (1,1,1), padding = 'same', kernel_regularizer = 'L2', name = 'conv3d_5')(x_4_bn_ac_pooling)\n",
        "    x_5_bn = tf.keras.layers.BatchNormalization(axis = -1)(x_5)\n",
        "    x_5_bn_ac = tf.keras.layers.Activation('relu')(x_5_bn)\n",
        "    x_5_pooling = tf.keras.layers.MaxPooling3D(strides = (2, 2, 2))(x_5_bn_ac)       \n",
        "    \n",
        "    gap_layer = tf.keras.layers.GlobalAveragePooling3D()(x_5_pooling) # Global average pooling layer\n",
        "    #model.add(tf.keras.layers.Dropout(0.3))\n",
        "    embeddings = tf.keras.layers.Dense(1024, activation = 'relu', kernel_regularizer = 'L2')(gap_layer)\n",
        "    \n",
        "    encoding_model = tf.keras.Model(inputs = input_layer, outputs =  embeddings, name = 'base_3dcnn')\n",
        "\n",
        "    # Siamese model starts\n",
        "\n",
        "    moving_input = tf.keras.Input(input_shape)\n",
        "    ref_input = tf.keras.Input(input_shape)\n",
        "\n",
        "    encoded_moving  = encoding_model(moving_input)\n",
        "    encoded_ref = encoding_model(ref_input)\n",
        "\n",
        "    L1_layer = tf.keras.layers.Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
        "    #L2_layer = tf.keras.layers.Lambda(lambda tensors:K.l2_normalize((tensors[0] - tensors[1]), axis = 1))\n",
        "\n",
        "    L1_distance = L1_layer([encoded_moving, encoded_ref]) # L1-norm\n",
        "    #dot_product = tf.keras.layers.dot([encoded_moving, encoded_ref], axes = 1, normalize = False)\n",
        "    #L2_distance = L2_layer([encoded_moving, encoded_ref]) # L2-norm or Euclidean Norm\n",
        "    #L2_distance = tf.keras.layers.Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([encoded_moving, encoded_ref])\n",
        "\n",
        "    prediction = tf.keras.layers.Dense(1, activation='sigmoid')(L1_distance)\n",
        "    siamesenet = tf.keras.Model(inputs = [moving_input, ref_input], outputs = prediction, name = 'siamese_3dmodel')\n",
        "    \n",
        "    return siamesenet, encoding_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eauk1cr6lW4T"
      },
      "source": [
        "img_shape = (30, 45, 30, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bsnxgx30hxUN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc9010a0-1945-4f68-af79-97ec38c05d0e"
      },
      "source": [
        "siamese_model, base_model = SiameseNetwork(img_shape)\n",
        "base_learning_rate = 0.00005\n",
        "base_model.summary()\n",
        "siamese_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"base_3dcnn\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 30, 45, 30, 1)]   0         \n",
            "_________________________________________________________________\n",
            "conv3d_1 (Conv3D)            (None, 30, 45, 30, 32)    896       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 30, 45, 30, 32)    128       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 30, 45, 30, 32)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_2 (Conv3D)            (None, 30, 45, 30, 32)    27680     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 30, 45, 30, 32)    128       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 30, 45, 30, 32)    0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d (MaxPooling3D) (None, 15, 22, 15, 32)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_3 (Conv3D)            (None, 15, 22, 15, 64)    55360     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 15, 22, 15, 64)    256       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 15, 22, 15, 64)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_4 (Conv3D)            (None, 15, 22, 15, 64)    110656    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 15, 22, 15, 64)    256       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 15, 22, 15, 64)    0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d_1 (MaxPooling3 (None, 7, 11, 7, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv3d_5 (Conv3D)            (None, 7, 11, 7, 256)     442624    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 7, 11, 7, 256)     1024      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 7, 11, 7, 256)     0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d_2 (MaxPooling3 (None, 3, 5, 3, 256)      0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling3d (Gl (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1024)              263168    \n",
            "=================================================================\n",
            "Total params: 902,176\n",
            "Trainable params: 901,280\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n",
            "Model: \"siamese_3dmodel\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 30, 45, 30,  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 30, 45, 30,  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "base_3dcnn (Functional)         (None, 1024)         902176      input_2[0][0]                    \n",
            "                                                                 input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot (Dot)                       (None, 1)            0           base_3dcnn[0][0]                 \n",
            "                                                                 base_3dcnn[1][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            2           dot[0][0]                        \n",
            "==================================================================================================\n",
            "Total params: 902,178\n",
            "Trainable params: 901,282\n",
            "Non-trainable params: 896\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JoIccFvh8JJ",
        "outputId": "5eda26f9-a4b5-45f3-81c9-ba77d584bca9"
      },
      "source": [
        "siamese_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = base_learning_rate), loss = 'binary_crossentropy', metrics = [accuracy, recall_m, specificity, precision_m, f1_m])\n",
        "    \n",
        "fine_tune_epochs = 20\n",
        "history_fine = siamese_model.fit([left_input_cv, right_input_cv], targets_cv, batch_size = 32,\n",
        "                              epochs = fine_tune_epochs,\n",
        "                              shuffle = True,\n",
        "                              validation_split = 0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "86/86 [==============================] - 148s 1s/step - loss: 13.5271 - accuracy: 0.5072 - recall_m: 0.5072 - specificity: 0.5072 - precision_m: 1.0000 - f1_m: 0.6681 - val_loss: 11.6540 - val_accuracy: 0.4927 - val_recall_m: 0.4927 - val_specificity: 0.4927 - val_precision_m: 1.0000 - val_f1_m: 0.6548\n",
            "Epoch 2/20\n",
            "86/86 [==============================] - 125s 1s/step - loss: 6.7797 - accuracy: 0.5065 - recall_m: 0.5065 - specificity: 0.5065 - precision_m: 1.0000 - f1_m: 0.6676 - val_loss: 9.3487 - val_accuracy: 0.4927 - val_recall_m: 0.4927 - val_specificity: 0.4927 - val_precision_m: 1.0000 - val_f1_m: 0.6548\n",
            "Epoch 3/20\n",
            "86/86 [==============================] - 126s 1s/step - loss: 6.4120 - accuracy: 0.5080 - recall_m: 0.5080 - specificity: 0.5080 - precision_m: 1.0000 - f1_m: 0.6694 - val_loss: 7.0183 - val_accuracy: 0.4927 - val_recall_m: 0.4927 - val_specificity: 0.4927 - val_precision_m: 1.0000 - val_f1_m: 0.6548\n",
            "Epoch 4/20\n",
            "86/86 [==============================] - 126s 1s/step - loss: 6.1095 - accuracy: 0.5065 - recall_m: 0.5065 - specificity: 0.5065 - precision_m: 1.0000 - f1_m: 0.6672 - val_loss: 6.0153 - val_accuracy: 0.4927 - val_recall_m: 0.4927 - val_specificity: 0.4927 - val_precision_m: 1.0000 - val_f1_m: 0.6548\n",
            "Epoch 5/20\n",
            "86/86 [==============================] - 125s 1s/step - loss: 5.8354 - accuracy: 0.5082 - recall_m: 0.5082 - specificity: 0.5082 - precision_m: 1.0000 - f1_m: 0.6694 - val_loss: 5.7130 - val_accuracy: 0.4927 - val_recall_m: 0.4927 - val_specificity: 0.4927 - val_precision_m: 1.0000 - val_f1_m: 0.6548\n",
            "Epoch 6/20\n",
            "86/86 [==============================] - 126s 1s/step - loss: 5.5838 - accuracy: 0.5070 - recall_m: 0.5070 - specificity: 0.5070 - precision_m: 1.0000 - f1_m: 0.6689 - val_loss: 5.4685 - val_accuracy: 0.4927 - val_recall_m: 0.4927 - val_specificity: 0.4927 - val_precision_m: 1.0000 - val_f1_m: 0.6548\n",
            "Epoch 7/20\n",
            "86/86 [==============================] - 126s 1s/step - loss: 5.3531 - accuracy: 0.5062 - recall_m: 0.5062 - specificity: 0.5062 - precision_m: 1.0000 - f1_m: 0.6665 - val_loss: 5.2457 - val_accuracy: 0.4927 - val_recall_m: 0.4927 - val_specificity: 0.4927 - val_precision_m: 1.0000 - val_f1_m: 0.6548\n",
            "Epoch 8/20\n",
            "86/86 [==============================] - 126s 1s/step - loss: 5.1402 - accuracy: 0.5070 - recall_m: 0.5070 - specificity: 0.5070 - precision_m: 1.0000 - f1_m: 0.6678 - val_loss: 5.0446 - val_accuracy: 0.4927 - val_recall_m: 0.4927 - val_specificity: 0.4927 - val_precision_m: 1.0000 - val_f1_m: 0.6548\n",
            "Epoch 9/20\n",
            "86/86 [==============================] - 126s 1s/step - loss: 4.9431 - accuracy: 0.5070 - recall_m: 0.5070 - specificity: 0.5070 - precision_m: 1.0000 - f1_m: 0.6675 - val_loss: 4.8618 - val_accuracy: 0.4927 - val_recall_m: 0.4927 - val_specificity: 0.4927 - val_precision_m: 1.0000 - val_f1_m: 0.6548\n",
            "Epoch 10/20\n",
            "15/86 [====>.........................] - ETA: 1:36 - loss: 4.8270 - accuracy: 0.5417 - recall_m: 0.5417 - specificity: 0.5417 - precision_m: 1.0000 - f1_m: 0.6996"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O6ZEkGGek_i"
      },
      "source": [
        "acc = history_fine.history['accuracy']\n",
        "val_acc = history_fine.history['val_accuracy']\n",
        "\n",
        "recall_m = history_fine.history['recall_m']\n",
        "val_recall_m = history_fine.history['val_recall_m']\n",
        "\n",
        "specificity = history_fine.history['specificity']\n",
        "val_specificity = history_fine.history['val_specificity']\n",
        "\n",
        "precision_m = history_fine.history['precision_m']\n",
        "val_precision_m = history_fine.history['val_precision_m']\n",
        "\n",
        "f1_m = history_fine.history['f1_m']\n",
        "val_f1_m = history_fine.history['val_f1_m']\n",
        "    \n",
        "loss = history_fine.history['loss']\n",
        "val_loss = history_fine.history['val_loss']\n",
        "    \n",
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1.01])\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(recall_m, label='Training Recall')\n",
        "plt.plot(val_recall_m, label='Validation Recall')\n",
        "plt.ylim([min(plt.ylim()),1.01])\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Recall(Sensitivity)')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(specificity, label='Training Specificity')\n",
        "plt.plot(val_specificity, label='Validation Specificity')\n",
        "plt.ylim([min(plt.ylim()),1.01])\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Specificity')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(precision_m, label='Training Precision')\n",
        "plt.plot(val_precision_m, label='Validation Precision')\n",
        "plt.ylim([min(plt.ylim()),1.01])\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Precision')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(f1_m, label='Training F1-score')\n",
        "plt.plot(val_f1_m, label='Validation F1-score')\n",
        "plt.ylim([min(plt.ylim()),1.01])\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('F1-score')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Contrastive Loss')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf1gDNzPf1sC"
      },
      "source": [
        "predicton_prob = siamese_model.predict([left_input_test, right_input_test])\n",
        "\n",
        "print(np.transpose(predicton_prob))\n",
        "\n",
        "predicton_prob[predicton_prob > 0.5] = 1\n",
        "predicton_prob[predicton_prob <= 0.5] = 0\n",
        "\n",
        "tn, fp, fn, tp = sklearn.metrics.confusion_matrix(targets_test, predicton_prob).ravel()\n",
        "\n",
        "print(f'test Accuracy: {sklearn.metrics.accuracy_score(targets_test, predicton_prob)}')\n",
        "print(f'test ROC (AUC): {sklearn.metrics.roc_auc_score(targets_test, predicton_prob)}')\n",
        "print(f'test Sensitivity (Recall): {sklearn.metrics.recall_score(targets_test, predicton_prob)}')\n",
        "print(f'test Precision: {sklearn.metrics.precision_score(targets_test, predicton_prob)}')\n",
        "print(f'test F1-score: {sklearn.metrics.f1_score(targets_test, predicton_prob)}')\n",
        "print(f'test Mathews Correlation Coefficient: {sklearn.metrics.matthews_corrcoef(targets_test, predicton_prob)}')\n",
        "print(f'test Specificity: {tn/(tn+fp)}')\n",
        "print(targets_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qpl6SQKPPRfi"
      },
      "source": [
        "# Save Model\n",
        "siamese_model.save('/content/drive/My Drive/Autism_CNN/autism_cnn_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgGLe9bl-een"
      },
      "source": [
        "siamese_model = tf.keras.models.load_model('/content/drive/My Drive/Autism_CNN/autism_cnn_model', custom_objects={'contrastive_loss':contrastive_loss, 'accuracy':accuracy, 'recall_m':recall_m, 'specificity': specificity, 'precision_m':precision_m, 'f1_m':f1_m})\n",
        "print('Model is loaded')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqP7_4vkAFIg"
      },
      "source": [
        "\n",
        "left_test = np.expand_dims(left_input_test[1], axis = (0, -1))\n",
        "print(left_test.shape)\n",
        "right_test = np.expand_dims(right_input_test[1], axis = (0, -1))\n",
        "#print(right_test.astype)\n",
        "p = siamese_model.predict([left_test, right_test])\n",
        "print(f'predicted percentage similarity is {(1-p[0][0])*100} and actual similarity is {(1-targets_test[1])*100}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjiNmvUxxzIQ"
      },
      "source": [
        "siamese_model.layers[-1].activation = None\n",
        "\n",
        "last_conv_layer_name = 'conv3d_5'\n",
        "last_layer = siamese_model.get_layer('base_3dcnn').get_layer(last_conv_layer_name)\n",
        "s_model = tf.keras.models.Model(\n",
        "        [siamese_model.inputs], [last_layer.output, siamese_model.output]\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiqB1n3yyD-g"
      },
      "source": [
        "with tf.GradientTape(persistent=True) as tape:\n",
        "    #tape.watch(s_model.trainable_variables)\n",
        "    #tape.watch(tf.Variable(left_test))\n",
        "    #tape.watch(tf.Variable(right_test))\n",
        "    last_conv_layer_output, prediction = s_model([left_test, right_test])\n",
        "    loss = prediction[:, 0]\n",
        "output = last_conv_layer_output[0]\n",
        "output_mean_map = tf.reduce_mean(output, axis=(0, 1, 2))\n",
        "print(loss)\n",
        "print(output_mean_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kclSWGXXymF1"
      },
      "source": [
        "grads = tape.gradient(loss, last_conv_layer_output, unconnected_gradients='zero')[0]\n",
        "print(grads.shape)\n",
        "weights = tf.reduce_mean(grads, axis = (0, 1, 2))\n",
        "print(weights)\n",
        "\n",
        "heat_map = np.zeros(output.shape[0:3], dtype=np.float32)\n",
        "\n",
        "for index, w in enumerate(weights):\n",
        "    heat_map += w * output[:, :, :, index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVNjTsSRNqoo"
      },
      "source": [
        "from skimage.transform import resize\n",
        "import cv2\n",
        "\n",
        "hm = resize(heat_map, (30, 45, 30))\n",
        "\n",
        "#heatmap = (hm - hm.min())/(hm.max() - hm.min())\n",
        "heatmap = hm\n",
        "\n",
        "f, axarr = plt.subplots(2,3, figsize=(15,10));\n",
        "f.suptitle('Grad-CAM')\n",
        "slice_count=14\n",
        "slice_count2=5\n",
        "\n",
        "mri_image = np.squeeze(right_test)\n",
        "\n",
        "sagittal_mri_img=np.rot90(np.squeeze(mri_image[slice_count, :, :]))\n",
        "sagittal_grad_cmap_img=np.squeeze(heatmap[slice_count, :, :])\n",
        "\n",
        "coronal_mri_img=np.rot90(np.squeeze(mri_image[:, slice_count2, :]))\n",
        "coronal_grad_cmap_img=np.squeeze(heatmap[:, slice_count2, :]) \n",
        "\n",
        "axial_mri_img=np.rot90(np.squeeze(mri_image[:, :, slice_count2]))\n",
        "axial_grad_cmap_img=np.squeeze(heatmap[:, :, slice_count2]) \n",
        "\n",
        "img_plot = axarr[0,0].imshow(axial_mri_img, cmap='gray');\n",
        "axarr[0,0].axis('off')\n",
        "axarr[0,0].set_title('MRI-Axial')\n",
        "    \n",
        "img_plot = axarr[0,1].imshow(coronal_mri_img, cmap='gray');\n",
        "axarr[0,1].axis('off')\n",
        "axarr[0,1].set_title('MRI-Coronal')\n",
        "\n",
        "img_plot = axarr[0,2].imshow(sagittal_mri_img, cmap='gray');\n",
        "axarr[0,2].axis('off')\n",
        "axarr[0,2].set_title('MRI-Sagittal')\n",
        "    \n",
        "#axial_overlay=cv2.addWeighted(axial_mri_img, 0.3, axial_grad_cmap_img, 0.6, 0)\n",
        "    \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}