{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Autism_siamese.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sutummala/AutismNet/blob/main/Autism_siamese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iq5akkk4CpnB",
        "outputId": "7f04336f-4834-4b4a-b312-78c46d48788b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7g0odBW6RA-"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "#import tensorflow_addons as tfa\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
        "import sklearn\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEuSRuMW6j15"
      },
      "source": [
        "# normalizing the input to have values between zero and one to make them suitable for further analysis\n",
        "def normalize(input):\n",
        "  norm_input = []\n",
        "  print(f'shape of input is {np.shape(input[0])}')\n",
        "  for i in range(np.shape(input)[0]):\n",
        "    norm_in = (input[i]-np.min(input[i]))/(np.max(input[i])-np.min(input[i]))\n",
        "    norm_input.append(norm_in)\n",
        "  return norm_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6gWYWB3g--B",
        "outputId": "c18f2881-0f81-41cd-bc3f-d96ff1e30075"
      },
      "source": [
        "left_input = np.load('/content/drive/My Drive/Autism_CNN/left_input.npy')\n",
        "left_input = np.squeeze(normalize(left_input))\n",
        "\n",
        "right_input = np.load('/content/drive/My Drive/Autism_CNN/right_input.npy')\n",
        "right_input = np.squeeze(normalize(right_input))\n",
        "\n",
        "targets = np.load('/content/drive/My Drive/Autism_CNN/autism_labels.npy')\n",
        "\n",
        "targets = 1-targets # 1 for negative pair and 0 for positive pair\n",
        "\n",
        "print(f'total size of the data is {len(targets)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of input is (30, 45, 30)\n",
            "shape of input is (30, 45, 30)\n",
            "total size of the data is 4280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZKLD3_hNIM7",
        "outputId": "6cbae0fa-e7b9-4823-c8fe-3feb10c1e23a"
      },
      "source": [
        "folds = RepeatedStratifiedKFold(n_splits = 5, n_repeats = 1)\n",
        "\n",
        "for train_index, test_index in folds.split(left_input, targets):\n",
        "    left_input_cv, left_input_test, targets_cv, targets_test = left_input[train_index], left_input[test_index], targets[train_index], targets[test_index]\n",
        "    right_input_cv, right_input_test = right_input[train_index], right_input[test_index]\n",
        "\n",
        "#cv_index = int(0.9 * len(targets)) \n",
        "\n",
        "#left_input_cv = left_input[:cv_index]\n",
        "#right_input_cv = right_input[:cv_index]\n",
        "#targets_cv = targets[:cv_index]\n",
        "\n",
        "print(f'shape of left/right input for CV is {left_input_cv.shape}')\n",
        "print(f'input size for cross-validation is {len(targets_cv)}')\n",
        "print(f'no.of positive pairs in CV are {np.shape(np.nonzero(targets_cv))[1]}')\n",
        "\n",
        "#left_input_test = left_input[cv_index:]\n",
        "#right_input_test = right_input[cv_index:]\n",
        "#targets_test = targets[cv_index:]\n",
        "\n",
        "print(f'shape of left/right input for testing is {left_input_test.shape}')\n",
        "print(f'input size for testing is {len(targets_test)}')\n",
        "print(f'no.of positive pairs in test are {np.shape(np.nonzero(targets_test))[1]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of left/right input for CV is (3424, 30, 45, 30)\n",
            "input size for cross-validation is 3424\n",
            "no.of positive pairs in CV are 1694\n",
            "shape of left/right input for testing is (856, 30, 45, 30)\n",
            "input size for testing is 856\n",
            "no.of positive pairs in test are 423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_Z6p5sLNprz"
      },
      "source": [
        "def specificity(y_true, y_pred):\n",
        "    \n",
        "    y_pred = K.cast(y_pred > 0.5, y_true.dtype)\n",
        "    tn, fp, fn, tp = (0, 0, 0, 0)\n",
        "    for i in range(len(y_true)):\n",
        "      if y_true[i] == 1 and y_pred[i] == 1:\n",
        "        tp += 1\n",
        "      elif y_true[i] == 1 and y_pred[i] == 0:\n",
        "        fp += 1\n",
        "      elif y_true[i] == 0 and y_pred[i] == 1:\n",
        "        fn += 1\n",
        "      elif y_true[i] == 0 and y_pred[i] == 0:\n",
        "        tn += 1\n",
        "    return tn/(tn+fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFHiMSJcQC9h"
      },
      "source": [
        "def contrastive_loss(y_true, y_pred):\n",
        "    margin = 1\n",
        "    square_pred = K.square(y_pred)\n",
        "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
        "    return K.mean((1-y_true) * square_pred + y_true * margin_square)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBFMSWEyNJ6g"
      },
      "source": [
        "## newly added methods begin\n",
        "def recall_m(y_true, y_pred):\n",
        "  y_pred = K.cast(y_pred > 0.5, y_true.dtype)\n",
        "  true_positives = (K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "  possible_positives = (K.round(K.clip(y_true, 0, 1)))\n",
        "  recall = K.mean(K.equal(true_positives, possible_positives))\n",
        "  return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "  y_pred = K.cast(y_pred > 0.5, y_true.dtype)\n",
        "  true_positives = (K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "  predicted_positives = (K.round(K.clip(y_pred, 0, 1)))\n",
        "  precision = K.mean(K.equal(true_positives, predicted_positives))\n",
        "  return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "  precision = precision_m(y_true, y_pred)\n",
        "  recall = recall_m(y_true, y_pred)\n",
        "  return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "##newly added end\n",
        "\n",
        "def compute_accuracy(y_true, y_pred):\n",
        "    '''Compute classification accuracy with a fixed threshold on distances.\n",
        "    '''\n",
        "    pred = y_pred.ravel() < 0.5\n",
        "    return np.mean(pred == y_true)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    '''Compute classification accuracy with a fixed threshold on distances.\n",
        "    '''\n",
        "    return K.mean(K.equal(y_true, K.cast(y_pred > 0.5, y_true.dtype)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B605Y_SfGXpL"
      },
      "source": [
        "def euclidean_distance(vectors):\n",
        "\t# unpack the vectors into separate lists\n",
        "\t(featsA, featsB) = vectors\n",
        "\t# compute the sum of squared distances between the vectors\n",
        "\tsumSquared = K.sum(K.square(featsA - featsB), axis=1,\n",
        "\t\tkeepdims=True)\n",
        "\t# return the euclidean distance between the vectors\n",
        "\treturn K.sqrt(K.maximum(sumSquared, K.epsilon()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTwj0FaeNStL"
      },
      "source": [
        "def eucl_dist_output_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0], 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbOUy2GR9TWn"
      },
      "source": [
        "def specificity1(y_true, y_pred):\n",
        "    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
        "    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n",
        "    return true_negatives / (possible_negatives + K.epsilon())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QYTMFZ8hbK7"
      },
      "source": [
        "def SiameseNetwork(input_shape):\n",
        "    \n",
        "    moving_input = tf.keras.Input(input_shape)\n",
        "    ref_input = tf.keras.Input(input_shape)\n",
        "    \n",
        "    #model = tf.keras.Sequential()\n",
        "    input = tf.keras.Input(input_shape)\n",
        "\n",
        "   # 1st 3D conv blocks, which involves, convolution, BN, activation and pooling \n",
        "    x_1 = tf.keras.layers.Conv3D(32, (3,3,3), strides = (1,1,1), padding = 'same', kernel_regularizer = 'L2', name = 'conv3d_1')(input)\n",
        "    x_1_bn = tf.keras.layers.BatchNormalization(axis = -1)(x_1)\n",
        "    x_1_bn_ac = tf.keras.layers.Activation('relu')(x_1_bn)\n",
        "\n",
        "    x_2 = tf.keras.layers.Conv3D(32, (3,3,3), strides = (1,1,1), padding = 'same', kernel_regularizer = 'L2', name = 'conv3d_2')(x_1_bn_ac)\n",
        "    x_2 = tf.keras.layers.BatchNormalization(axis = -1)(x_2)\n",
        "    x_2 = tf.keras.layers.Activation('relu')(x_2)\n",
        "    x_2 = tf.keras.layers.MaxPooling3D(strides = (2, 2, 2))(x_2)\n",
        "              \n",
        "    # 2nd 3D conv block, which involves, convolution, BN, activation and pooling \n",
        "    x_3 = tf.keras.layers.Conv3D(64, (3,3,3), strides = (1,1,1), padding = 'same', kernel_regularizer = 'L2', name = 'conv3d_3')(x_2)\n",
        "    x_3 = tf.keras.layers.BatchNormalization(axis = -1)(x_3)\n",
        "    x_3 = tf.keras.layers.Activation('relu')(x_3)\n",
        "\n",
        "    x_4 = tf.keras.layers.Conv3D(64, (3,3,3), strides = (1,1,1), padding = 'same', kernel_regularizer = 'L2', name = 'conv3d_4')(x_3)\n",
        "    x_4 = tf.keras.layers.BatchNormalization(axis = -1)(x_4)\n",
        "    x_4 = tf.keras.layers.Activation('relu')(x_4)\n",
        "    x_4 = tf.keras.layers.MaxPooling3D(strides = (2, 2, 2))(x_4)\n",
        "    \n",
        "    # 3rd 3D conv block, which involves, convolution, BN, activation and pooling \n",
        "    x_5 = tf.keras.layers.Conv3D(256, (3,3,3), strides = (1,1,1), padding = 'same', kernel_regularizer = 'L2', name = 'conv3d_5')(x_4)\n",
        "    x_5_bn = tf.keras.layers.BatchNormalization(axis = -1)(x_5)\n",
        "    x_5_bn_ac = tf.keras.layers.Activation('relu')(x_5_bn)\n",
        "    x_5_pooling = tf.keras.layers.MaxPooling3D(strides = (2, 2, 2))(x_5_bn_ac)       \n",
        "    \n",
        "    gap_layer = tf.keras.layers.GlobalAveragePooling3D()(x_5_pooling)\n",
        "    #model.add(tf.keras.layers.Dropout(0.3))\n",
        "    dense_layer = tf.keras.layers.Dense(1024, activation = 'relu', kernel_regularizer = 'L2')(gap_layer)\n",
        "    \n",
        "    encoding_model = tf.keras.Model(inputs = input, outputs =  dense_layer)\n",
        "\n",
        "    encoded_moving  = encoding_model(moving_input)\n",
        "    encoded_ref = encoding_model(ref_input)\n",
        "\n",
        "    L1_layer = tf.keras.layers.Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
        "    #L2_layer = tf.keras.layers.Lambda(lambda tensors:K.l2_normalize((tensors[0] - tensors[1]), axis = 1))\n",
        "\n",
        "    L1_distance = L1_layer([encoded_moving, encoded_ref]) # L1-norm\n",
        "    #L2_distance = L2_layer([encoded_moving, encoded_ref]) # L2-norm or Euclidean Norm\n",
        "    #L2_distance = tf.keras.layers.Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([encoded_moving, encoded_ref])\n",
        "\n",
        "    prediction = tf.keras.layers.Dense(1, activation='sigmoid')(L1_distance)\n",
        "    siamesenet = tf.keras.Model(inputs = [moving_input, ref_input], outputs = prediction)\n",
        "    \n",
        "    return siamesenet, encoding_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eauk1cr6lW4T"
      },
      "source": [
        "img_shape = (30, 45, 30, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bsnxgx30hxUN",
        "outputId": "cc9f4d31-e4ef-4288-e8b8-d617d3d80a41"
      },
      "source": [
        "siamese_model, base_model = SiameseNetwork(img_shape)\n",
        "base_learning_rate = 0.00005\n",
        "base_model.summary()\n",
        "siamese_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 30, 45, 30, 1)]   0         \n",
            "_________________________________________________________________\n",
            "conv3d_1 (Conv3D)            (None, 30, 45, 30, 32)    896       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 30, 45, 30, 32)    128       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 30, 45, 30, 32)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_2 (Conv3D)            (None, 30, 45, 30, 32)    27680     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 30, 45, 30, 32)    128       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 30, 45, 30, 32)    0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d (MaxPooling3D) (None, 15, 22, 15, 32)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_3 (Conv3D)            (None, 15, 22, 15, 64)    55360     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 15, 22, 15, 64)    256       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 15, 22, 15, 64)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_4 (Conv3D)            (None, 15, 22, 15, 64)    110656    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 15, 22, 15, 64)    256       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 15, 22, 15, 64)    0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d_1 (MaxPooling3 (None, 7, 11, 7, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv3d_5 (Conv3D)            (None, 7, 11, 7, 256)     442624    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 7, 11, 7, 256)     1024      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 7, 11, 7, 256)     0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d_2 (MaxPooling3 (None, 3, 5, 3, 256)      0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling3d (Gl (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1024)              263168    \n",
            "=================================================================\n",
            "Total params: 902,176\n",
            "Trainable params: 901,280\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 30, 45, 30,  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 30, 45, 30,  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model (Functional)              (None, 1024)         902176      input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 1024)         0           model[0][0]                      \n",
            "                                                                 model[1][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            1025        lambda[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 903,201\n",
            "Trainable params: 902,305\n",
            "Non-trainable params: 896\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JoIccFvh8JJ",
        "outputId": "84bb1880-4a0b-4877-e7b9-9f0b8c8496b2"
      },
      "source": [
        "siamese_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = base_learning_rate), loss = contrastive_loss, metrics = [accuracy, recall_m, specificity, precision_m, f1_m])\n",
        "    \n",
        "fine_tune_epochs = 2\n",
        "history_fine = siamese_model.fit([left_input_cv, right_input_cv], targets_cv, batch_size = 32,\n",
        "                              epochs = fine_tune_epochs,\n",
        "                              shuffle = True,\n",
        "                              validation_split = 0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "86/86 [==============================] - 3438s 40s/step - loss: 6.2849 - accuracy: 0.5158 - recall_m: 0.7639 - specificity: nan - precision_m: 0.7519 - f1_m: 0.7245 - val_loss: 5.7912 - val_accuracy: 0.4762 - val_recall_m: 0.9262 - val_specificity: nan - val_precision_m: 0.5499 - val_f1_m: 0.6841\n",
            "Epoch 2/2\n",
            " 4/86 [>.............................] - ETA: 51:42 - loss: 5.7696 - accuracy: 0.5703 - recall_m: 0.6562 - specificity: 0.5614 - precision_m: 0.9141 - f1_m: 0.7626"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O6ZEkGGek_i"
      },
      "source": [
        "acc = history_fine.history['accuracy']\n",
        "val_acc = history_fine.history['val_accuracy']\n",
        "\n",
        "recall_m = history_fine.history['recall_m']\n",
        "val_recall_m = history_fine.history['val_recall_m']\n",
        "\n",
        "specificity = history_fine.history['specificity']\n",
        "val_specificity = history_fine.history['val_specificity']\n",
        "\n",
        "precision_m = history_fine.history['precision_m']\n",
        "val_precision_m = history_fine.history['val_precision_m']\n",
        "\n",
        "f1_m = history_fine.history['f1_m']\n",
        "val_f1_m = history_fine.history['val_f1_m']\n",
        "    \n",
        "loss = history_fine.history['loss']\n",
        "val_loss = history_fine.history['val_loss']\n",
        "    \n",
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1.01])\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(recall_m, label='Training Recall')\n",
        "plt.plot(val_recall_m, label='Validation Recall')\n",
        "plt.ylim([min(plt.ylim()),1.01])\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Recall(Sensitivity)')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(specificity, label='Training Specificity')\n",
        "plt.plot(val_specificity, label='Validation Specificity')\n",
        "plt.ylim([min(plt.ylim()),1.01])\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Specificity')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(precision_m, label='Training Precision')\n",
        "plt.plot(val_precision_m, label='Validation Precision')\n",
        "plt.ylim([min(plt.ylim()),1.01])\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Precision')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(f1_m, label='Training F1-score')\n",
        "plt.plot(val_f1_m, label='Validation F1-score')\n",
        "plt.ylim([min(plt.ylim()),1.01])\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('F1-score')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Contrastive Loss')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf1gDNzPf1sC"
      },
      "source": [
        "predicton_prob = siamese_model.predict([left_input_test, right_input_test])\n",
        "\n",
        "print(np.transpose(predicton_prob))\n",
        "\n",
        "predicton_prob[predicton_prob > 0.5] = 1\n",
        "predicton_prob[predicton_prob <= 0.5] = 0\n",
        "\n",
        "tn, fp, fn, tp = sklearn.metrics.confusion_matrix(targets_test, predicton_prob).ravel()\n",
        "\n",
        "print(f'test Accuracy: {sklearn.metrics.accuracy_score(targets_test, predicton_prob)}')\n",
        "print(f'test ROC (AUC): {sklearn.metrics.roc_auc_score(targets_test, predicton_prob)}')\n",
        "print(f'test Sensitivity (Recall): {sklearn.metrics.recall_score(targets_test, predicton_prob)}')\n",
        "print(f'test Precision: {sklearn.metrics.precision_score(targets_test, predicton_prob)}')\n",
        "print(f'test F1-score: {sklearn.metrics.f1_score(targets_test, predicton_prob)}')\n",
        "print(f'test Mathews Correlation Coefficient: {sklearn.metrics.matthews_corrcoef(targets_test, predicton_prob)}')\n",
        "print(f'test Specificity: {tn/(tn+fp)}')\n",
        "print(targets_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qpl6SQKPPRfi"
      },
      "source": [
        "# Save Model\n",
        "siamese_model.save('/content/drive/My Drive/Autism_CNN/autism_cnn_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgGLe9bl-een"
      },
      "source": [
        "loaded_siamese_model = tf.keras.models.load_model('/content/drive/My Drive/Autism_CNN/autism_cnn_model', custom_objects={'contrastive_loss':contrastive_loss, 'accuracy':accuracy, 'recall_m':recall_m, 'specificity': specificity, 'precision_m':precision_m, 'f1_m':f1_m})\n",
        "print('Model is loaded')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqP7_4vkAFIg"
      },
      "source": [
        "\n",
        "left_test = np.expand_dims(left_input_test[0], axis = (0, -1))\n",
        "print(left_test.shape)\n",
        "right_test = np.expand_dims(right_input_test[0], axis = (0, -1))\n",
        "p = siamese_model.predict([left_test, right_test])\n",
        "print(f'predicted probability is {p} and actual probability is {targets_test[0]}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}